---
output: html_document
---

<center>
# XGBOOST - Econometría I
#### Agustin Riquelme y Heriberto Espino
</center>
</n>

# 1. Introducción

XGBoost (eXtreme Gradient Boosting) es una libreria incorporada en R que implementa algoritmos de aprendizaje automático con árboles de decisión para resolver problemas de regresión y clasificación, así como tareas de rankeo. Algunas características y aplicaciones para este método son:

* Boosting: Con ayuda de modelos débiles (árbole de decisión), crea modelos más fuertes y los mejora iterativamente a medida que asigna pesos a errores y ajusta los modelos siguientes para corregir dichos errores

* Regularización: Usa técnicas que ayudan a evitar un sobreajuste del modelo, además de poder controlar la tasa de aprendizaje, la profundidad del árbol y la cantidad de muestras requeridas para aumentar su complejidad

* Manejo de datos faltantes: Es capaz de manejar los NA al poder construir árboles de decisión incluso con ausencia de datos, ayudando a su robustez

* Importancia de las características: Proporciona medidas de importancia para comprender las características con mayor relevancia dentro del modelo

* Eficiencia: Este modelo está diseñado para aprovechar al máximo el uso de recursos y así reduciendo el tiempo en el que el modelo trabaja, entrenando conjuntos de datos enormes de manera rápida

La base de datos utilizada fue obtenida de la página https://archive.ics.uci.edu/dataset/27/credit+approval y contiene datos de aplicaciones de tarjetas de crédito, los nombres y algunos valores fueron cambiados para la protección de datos. Podemos encontrar que las variables son: genero, nominal; edad, entero; tiempo promedio en vivienda, continuo; estado de casa, nominal; ocupación, nominal; estado de trabajo actual, nominal; tiempo promedio con empleadores, continuo; otras inversiones, continio; cuenta bancaria, continuo; tiempo con el banco, continuo; referencia de responsabilidad, nominal; referencia de cuenta, nominal; gasto mensual de vivienda, continuo; ahorros, continuo; clasem nominal.

# 2. Planteamiento del problema de aplicación

El objetivo principal de este análisis es desarrollar un modelo de predicción utilizando la técnica de boosted trees XGBoost para predecir si una solicitud de tarjeta de crédito será aprobada o no.

La predicción de aprobación de crédito es una tarea fundamental en el sector financiero, determinando la concesión responsable para clientes calificados y la mitigación de riesgo crediticio. Dado el incremento exponencial de datos disponibles y el auge de técnicas avanzadas de aprendizaje automático, existe una valiosa oportunidad para desarrollar sistemas automatizados de apoyo a estas decisiones.

La base de datos disponible contiene información anonimizada de 689 solicitudes previas con 15 atributos sociodemográficos, financieros y de comportamiento crediticio. La variable objetivo es dicotómica, codificando si dicha solicitud fue aprobada o no

# 3. Implementación en R

```{r message=FALSE, warning=FALSE}
credit <- read.csv("C:/Users/herie/Downloads/credit+approval/crx.data")
library(PerformanceAnalytics)
library(caret)
library(xgboost)
library(ggplot2)
library(DiagrammeR)
str(credit)
```

## 3.1. Limpieza de la base de datos

Para poder realizar este modelo se realizará una limpieza de la base donde se eliminará una columna que tiene los mismos datos que otra, esto se supo al obtener una recta identidad entre sus datos-

```{r}
colnames(credit) <- c("sex", "age", "mtadress", "homestat", "lomismo", 
                      "occupation", "jobstatus", "mtemployers", "othinv", 
                      "bankacc", "twbank", "liabilityred", "accref", 
                      "mhexpense", "savingsbal","class")
table(credit$lomismo, credit$homestat)
credit <- credit[ , -5]
```

Ahora deberemos transformar aquellos valores numéricos que en nuestros datos se muestran como caracteres, así como los datos enteros deben transformarse a tipo double ya que el modelo únicamente trabaja con estos datos. Además, aquellos valores de class que se representan con el signo de adición deben ser cambiados por valores numéricos como 0 y 1

```{r warning=FALSE}
str(credit)
credit$class = ifelse(credit$class == "+", 1, 0)
credit$age = as.numeric(credit$age)
credit$mhexpense = as.numeric(credit$mhexpense)
credit$savingsbal = as.double(credit$savingsbal)
credit$twbank = as.double(credit$twbank)
```

De ahí, a pesar de no necesitarse, se optó por eliminar los valores nulos, entonces aquellos valores que sean "?", se llenarán con NA y se completarán los casos faltantes

```{r}
credit[credit == "?"] <- NA
credit = credit[complete.cases(credit), ]   
```

Para poder observar mejor los datos numéricos, vamos a recuperar dichos datos con un filtro para aquellos valores numéricos y de ellos vamos a eliminar el dato clase ya que corresponde a un dato cualitativo transformado en valores binarios. Para observar aquellos valores numéricos mejor, se realizarán histogramas y boxplots para dichos datos, además al final se visualizará un gráfico de correlación para dichos datos.

```{r warning=FALSE}
data_numeric = Filter(is.numeric, credit)
data_numeric = data_numeric[ , -7]
summary(data_numeric)
par(mfrow = c(2, 3))
  for (i in colnames(data_numeric)) {
    hist(credit[ , i], main = i)}
  for (i in colnames(data_numeric)) {
    boxplot(credit[ , i], main = i)}
par(mfrow = c(1, 1))
chart.Correlation(data_numeric)
```

Para realizar las predicciones se necesitará de igual manera preparar variables al transformarlas a dummies para un análisis correcto y posteriormente se separarán las variables en train y test para poder entrenar nuestro modelo y realizar predicciones, el entrenamiento será con el 80% de los datos y con los restantes se probará nuestro modelo. Todos estos datos serán manejados con una matriz.

```{r}
data_dummy = model.matrix(class ~ . -1, data = credit)
trainindex <- createDataPartition(credit$class, p = .80,
                                  list = FALSE, times = 1)
head(trainindex)
data_train = data_dummy[trainindex, ]
credit_train = credit$class[trainindex]
data_test = data_dummy[-trainindex, ]
credit_test = credit$class[-trainindex]
```

## 3.2. Modelo a través de errores

Para nuestro primer modelo de xgboost se tendrá una tasa de aprendizaje de 30%, 6 como número de ramas, le indicaremos que la muestra será obtenida uniformemente, la regularización hará que los pesos más altos se conserven más. Esto se realizará con 50 pasos y además la métrica que vamos a evaluar será a través de los errores.

```{r}
xgbst_error <- xgboost(data = data_train,                                       # base de datos
                       eta = 0.3,                                               # learning rate, (0,1)
                       gamma = 0,                                               # dividir nodo, 0 mas flexible
                       max.depth = 6,                                           # numero de ramas
                       min_child_weight = 1,                                    # peso necesario para que el nodo se divida
                       max_delta_step = 0,                                      # cuanto puede cambiar la prediccion en cada paso
                       subsample = 1,                                           # datos aleatorios que va a agarrar para predecir
                       sampling_method = "uniform",
                       lambda = 1,                                              # regularización de los pesos, mas grande - mas conservativo
                       alpha = 0,                                               # igual que lambda
                       tree_method = "auto",                                    # ver pagina
                       label = credit_train,                                    # variable que vamos a predecir
                       nrounds = 50,                                            # numero de pasos
                       objective = "binary:logistic",                           # queremos predecir eseto
                       eval.metric = 'error' ,                                  # se centra en los errores
                       verbose = 1)                                             # imprime mensaje de warning  
```

Graficando dichos errores pueden ser observados en la siguiente gráfica donde vemos que a lo largo de las iteraciones, el error va disminuyendo, además, seguida de ella podremos observar los árboles de decisión creados por nuestro modelo, nos limitamos a mostrar el primer árbol creado y el último para observar los cambios a lo largo de las simulaciones realizadas en el modelo, el primer árbol suele tener el modelo menos completo que el último, incluso a pesar de que el primero parece tener más ramas y hojas

```{r}
ggplot(data = xgbst_error$evaluation_log) +
  geom_line(aes(x = iter, y = train_error), color = "red") 
```

```{r}
xgb.plot.tree(model = xgbst_error, trees = 0)                                   # imprime el primer arbol
xgb.plot.tree(model = xgbst_error, trees = 49)                                  # imprime el ultimo arbol
```

Ahora para poder observar la importancia de las variables, nos ayudaremos de la funcion xgb.importance donde obtendremos en una tabla las 10 variables con más importancia y en nuestra gráfica de barras podremos ver todas las variables más gráficamente en órden de importancia. Podemos observar que en este caso othinvt es aquella que tiene la mayor importancia y supera por demasiado a todas las demás

```{r}
importance <- xgb.importance(feature_names = colnames(data_train), 
                             model = xgbst_error)
head(importance, n = 10)
xgb.plot.importance(importance_matrix = importance)
```

Ahora realizaremos las predicciones con nuestro modelo ya entrenado con los datos de train, la certeza de dichas predicciones pueden ser vistas en la matriz de confusión  donde podemos observar una precisión de 87%

```{r}
credit_pred <- predict(xgbst_error, data_test)
credit_pred <- as.numeric(credit_pred > 0.5)
caret::confusionMatrix(table(credit_pred, credit_test), positive=NULL)
```

Y al evaluar el modelo para observar la diferencia entre los valores de la predicción y del entrenamiento, donde ambos deberían de tener un comportamiento simialr

```{r}
xgbst_error_cv <- xgb.cv(data = data_train,                                     # base de datos
                         nfold = 10,
                         eta = 0.3,                                             # learning rate, (0,1)
                         gamma = 0,                                             # dividir nodo, 0 mas flexible
                         max.depth = 6,                                         # numero de ramas
                         min_child_weight = 1,                                  # peso necesario para que el nodo se divida
                         max_delta_step = 0,                                    # cuanto puede cambiar la prediccion en cada paso
                         subsample = 1,                                         # datos aleatorios que va a agarrar para predecir
                         sampling_method = "uniform",
                         lambda = 1,                                            # regularización de los pesos, mas grande - mas conservativo
                         alpha = 0,                                             # igual que lambda
                         tree_method = "auto",                                  # ver pagina
                         label = credit_train,                                  # variable que vamos a predecir
                         nrounds = 50,                                          # numero de pasos
                         objective = "binary:logistic",                         # queremos predecir eseto
                         eval.metric = 'error' ,                                # se centra en los errores
                         verbose = 1)                                           # imprime mensaje de warning  

ggplot(xgbst_error_cv$evaluation_log) + 
  geom_line(aes(iter, train_error_mean), color = "red") +
  geom_line(aes(iter, test_error_mean), color = "blue") 
```

Como podemos observar después de cierta iteración, el error para nuestro modelo se vuelve cero, sin emabrgo, el test de nuestro modelo no obtiene un error igual a cero conforme las iteraciones pasan, indicando que nuestro modelo resultó sobrenetrenado, para ello recurriremos a otro método métrico de evaluación para obtener un mejor modelo.

## 3.3. Modelo a través de logloss

Ahora, para nuestro segundo modelo se utilizaron en sí los mismo datos que en el primero pero en este caso lo que cambia viene siendo la métrica de evaluación, que en vez de realizarla por errores, se realiza a partir de logloss donde la pérdida de registros penaliza las clasificaciones falsas teniendo en cuenta la probabilidad de clasificación

```{r}
xgbst_logloss <- xgboost(data = data_train,                                     # base de datos
                       eta = 0.3,                                               # learning rate, (0,1)
                       gamma = 0,                                               # dividir nodo, 0 mas flexible
                       max.depth = 6,                                           # numero de ramas
                       min_child_weight = 1,                                    # peso necesario para que el nodo se divida
                       max_delta_step = 0,                                      # cuanto puede cambiar la prediccion en cada paso
                       subsample = 1,                                           # datos aleatorios que va a agarrar para predecir
                       sampling_method = "uniform",
                       lambda = 1,                                              # regularización de los pesos, mas grande - mas conservativo
                       alpha = 0,                                               # igual que lambda
                       tree_method = "auto",                                    # ver pagina
                       label = credit_train,                                    # variable que vamos a predecir
                       nrounds = 50,                                            # numero de pasos
                       objective = "binary:logistic",                           # queremos predecir eseto
                       eval.metric = 'logloss' ,                                # metodo logloss
                       verbose = 1)                                             # imprime mensaje de warning  

```

Y en la gráfica de la evolución de los errores podemos observar que los errores sí van disminuyendo conforme las simulaciones van aumentando, sin embargo, el error parece mantenerse por encima del cero, indicando que, a comparación del anterior, este no está sobre entrenado. Además, podemos observar que los árboles de decisión son muy similares comparados con los del modelo anterior

```{r}
ggplot(xgbst_logloss$evaluation_log) + 
  geom_line(aes(iter, train_logloss), color = "red")
```

```{r}
xgb.plot.tree(model = xgbst_logloss, trees = 0)
xgb.plot.tree(model = xgbst_logloss, trees = 49) 
```

Para las variables que tienen mayor impacto sobre el modelo, de igual manera domina othinvt sobre todas las demás, pudiendo concluir de ambos modelos que es una variable bastante significativa para predecir si el crédito es aprobado o no

```{r}
importance <- xgb.importance(feature_names = colnames(data_train), 
                             model = xgbst_logloss)
head(importance, n = 10)
xgb.plot.importance(importance_matrix = importance)
```

Para evaluar el desempeño de nuestro nuevo modelo, nuevamente realizaremos nuestra matriz de confusión pero con los datos más recientes, donde podemos observar que incluso después de no sobrentrenar el modelo, nuestra precisión disminuyó ligeramente.

```{r}
credit_pred <- predict(xgbst_logloss, data_test)
credit_pred <- as.numeric(credit_pred > 0.5)
caret::confusionMatrix(table(credit_pred, credit_test), positive=NULL)
```

En nuestra función de validación cruzada, aplicada anteriormente y cuya función es evaluar el rendimiento del modelo, reducir el riesgo de sobreajuste y además validar el modelo en diferentes subconjuntos de datos obtenemos que nuestro nuevo modelo tiene un mejor ajuste pero sigue existiendo un sobreajuste ya que la línea de entrenamiento disminuye pero la línea de prueba aumenta

```{r}
xgbst_logloss_cv <- xgb.cv(data = data_train,                                   # base de datos
                         nfold = 10,
                         eta = 0.3,                                             # learning rate, (0,1)
                         gamma = 0,                                             # dividir nodo, 0 mas flexible
                         max.depth = 6,                                         # numero de ramas
                         min_child_weight = 1,                                  # peso necesario para que el nodo se divida
                         max_delta_step = 0,                                    # cuanto puede cambiar la prediccion en cada paso
                         subsample = 1,                                         # datos aleatorios que va a agarrar para predecir
                         sampling_method = "uniform",
                         lambda = 1,                                            # regularización de los pesos, mas grande - mas conservativo
                         alpha = 0,                                             # igual que lambda
                         tree_method = "auto",                                  # ver pagina
                         label = credit_train,                                  # variable que vamos a predecir
                         nrounds = 50,                                          # numero de pasos
                         objective = "binary:logistic",                         # queremos predecir eseto
                         eval.metric = 'logloss' ,                              # se centra en los errores
                         verbose = 1)                                           # imprime mensaje de warning  

ggplot(xgbst_logloss_cv$evaluation_log) + 
  geom_line(aes(iter, train_logloss_mean), color = "red") +
  geom_line(aes(iter, test_logloss_mean), color = "blue")
```

## 3.4. Mejora del modelo

Ahora, para nuestro nuevo modelo cambiaremos varios valores, tal como la tasa de aprendizaje, la cual disminuye de 0.3 a 0.1, el gamma que aumenta de 0 a 1, la cantidad de ramas cambiando de 6 a 8, la subsample pasando de 1 a 0.8 y el numero rondas pasa de 50 a 100, con este nuevo ajuste, los valores de error son un poquito más grandes, que podrían parecer malos para el modelo, pero posteriormente observaremos nuestras otras métricas 

```{r}
xgbst_logloss <- xgboost(data = data_train,                                     # base de datos
                         eta = 0.1,                                             # learning rate, (0,1)
                         gamma = 1,                                             # dividir nodo, 0 mas flexible
                         max.depth = 8,                                         # numero de ramas
                         min_child_weight = 1,                                  # peso necesario para que el nodo se divida
                         max_delta_step = 0,                                    # cuanto puede cambiar la prediccion en cada paso
                         subsample = 0.8,                                       # datos aleatorios que va a agarrar para predecir
                         sampling_method = "uniform",
                         lambda = 1,                                            # regularización de los pesos, mas grande - mas conservativo
                         alpha = 0,                                             # igual que lambda
                         tree_method = "auto",                                  # ver pagina
                         label = credit_train,                                  # variable que vamos a predecir
                         nrounds = 100,                                         # numero de pasos
                         objective = "binary:logistic",                         # queremos predecir eseto
                         eval.metric = 'logloss' ,                              # metodo logloss
                         verbose = 1)                                           # imprime mensaje de warning  
```

Al evaluar el modelo, la gráfica que obtenemos tiene el mismo comportamiento, donde disminuye conforme las iteraciones aumentan, sin embargo, en los árboles de decisión sí cambia en bastante medida su comportamiento, el primer árbol tiene más ramificaciones y hojas y nuestro árbol final queda muy reducido. Porteriormente observaremos su validación

```{r}
ggplot(xgbst_logloss$evaluation_log) + 
  geom_line(aes(iter, train_logloss), color = "red")
```

```{r}
xgb.plot.tree(model = xgbst_logloss, trees = 0)
xgb.plot.tree(model = xgbst_logloss, trees = 49) 
```

Para la importancia de las variables, observamos lo mismo que los pasados, realmente esta variable no cambia conforme a los modelos. Sin embargo, observamos que para la precisión del modelo, obtenemos un poquito mejor de precisión incluso comparado con el primer modelo

```{r}
importance <- xgb.importance(feature_names = colnames(data_train), 
                             model = xgbst_logloss)
head(importance, n = 10)
xgb.plot.importance(importance_matrix = importance)
```

```{r}
credit_pred <- predict(xgbst_logloss, data_test)
credit_pred <- as.numeric(credit_pred > 0.5)
caret::confusionMatrix(table(credit_pred, credit_test), positive=NULL)
```

Para la evaluación del modelo podemos observar que a diferencia de los modelos pasados, las líneas de los errores al final de las iteraciones para test y train se mantienen paralelas a pesar de estar separadas, pudiendo concluir que este último modelo donde cambiamos distintos parametros, existe una estabilización indicando que el modelo ha alcanzado su capacidad máxima de aprendizaje

```{r}
xgbst_logloss_cv <- xgb.cv(data = data_train,                                   # base de datos
                           nfold = 10,
                           eta = 0.1,                                           # learning rate, (0,1)
                           gamma = 1,                                           # dividir nodo, 0 mas flexible
                           max.depth = 7,                                       # numero de ramas
                           min_child_weight = 1,                                # peso necesario para que el nodo se divida
                           max_delta_step = 0,                                  # cuanto puede cambiar la prediccion en cada paso
                           subsample = 0.8,                                     # datos aleatorios que va a agarrar para predecir
                           sampling_method = "uniform",
                           lambda = 1,                                          # regularización de los pesos, mas grande - mas conservativo
                           alpha = 0,                                           # igual que lambda
                           tree_method = "auto",                                # ver pagina
                           label = credit_train,                                # variable que vamos a predecir
                           nrounds = 50,                                        # numero de pasos
                           objective = "binary:logistic",                       # queremos predecir eseto
                           eval.metric = 'logloss' ,                            # se centra en los errores
                           verbose = 1)                                         # imprime mensaje de warning  


ggplot(xgbst_logloss_cv$evaluation_log) + 
  geom_line(aes(iter, train_logloss_mean), color = "red") +
  geom_line(aes(iter, test_logloss_mean), color = "blue") 
```

## 4. Conclusiones

El modelo de árboles de decisión XGBoost implementado demostró ser una herramienta útil para la predicción de aprobación de créditos a partir de los datos proporcionados. Mediante distintas iteraciones de entrenamiento y validación, se lograron encontrar los parámetros ideales para maximizar la precisión predictiva sin caer en sobreentrenamiento.

La variable más relevante resultó ser "othinvt", relacionada con otras inversiones, superando por mucho en importancia al resto de predictores. Otras variables destacadas fueron los gastos mensuales de vivienda, el balance de ahorros y el tiempo promedio en una dirección.

Inicialmente se utilizó la métrica de error para entrenar el modelo, lo cual resultó en sobreentrenamiento al alcanzar error cero. Posteriormente se cambió a la métrica log-loss, que penaliza más los falsos positivos de acuerdo a su probabilidad. Asimismo, se ajustaron parámetros como disminuir la tasa de aprendizaje y aumentar la complejidad.

El modelo final, con dichos ajustes, alcanzó una precisión de 89.2% y curvas de error de entrenamiento y prueba estabilizadas, indicativo de buen ajuste sin sobreentrenar. Por tanto, XGBoost probó ser una técnica prometedora para elaborar sistemas de decisión automáticos aplicando aprendizaje de máquina a bases de datos reales. Queda como trabajo futuro explorar nuevas mejoras en parámetros y comparación con otros algoritmos.

## 5. Referencias

* Amazon Web Services, Inc. (2023). Tutorial: Creación de modelos XGBoost - Amazon Redshift. Recuperado de https://docs.aws.amazon.com/es_es/redshift/latest/dg/tutorial_xgboost.html

* Esri. (2023). Cómo funciona el algoritmo XGBoost—ArcGIS Pro | Documentación. Recuperado de https://pro.arcgis.com/es/pro-app/latest/tool-reference/geoai/how-xgboost-works.htm

* ResearchGate. (2023). Credit Approval Dataset: List of Attributes. Recuperado de https://www.researchgate.net/figure/Credit-Approval-Dataset-List-of-Attributes_tbl1_3297254

* XGBoost Documentation. (2022). XGBoost 2.0.2 documentation. Recuperado de https://xgboost.readthedocs.io/en/stable/

